[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "pincite-evals"
version = "0.1.0"
description = "Evaluation harness scaffold for legal drafting with pinpoint citations"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [{name = "OpenAI Solutions", email = "noreply@example.com"}]
dependencies = [
  "openai>=2.17.0",
  "pydantic>=2.0.0",
  "PyYAML>=6.0.0",
  "python-dotenv>=1.0.0",
  "tqdm>=4.66.0",
  "pandas>=2.0.0",
  "PyMuPDF>=1.24.0",
  "certifi>=2024.0.0",
  "tiktoken>=0.8.0",
  "tenacity>=8.2.0",
  "plotly>=5.22.0",
  "kaleido>=1.0.0",
  "Jinja2>=3.1.0",
  "streamlit>=1.41.0",
]

[project.optional-dependencies]
dev = [
  "pytest>=8.0.0",
]

[project.scripts]
pincite-eval = "pincite_evals.eval_runner.runner:main"
pincite-synth = "pincite_evals.synthetic_generation.cli:main"
pincite-review-dashboard = "pincite_evals.experiment_review_dashboard:launch"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
"pincite_evals.eval_runner" = ["prom/**/*.txt"]
"pincite_evals.synthetic_generation" = ["prompts/**/*.txt"]
"pincite_evals.graders" = ["prompts/**/*.txt"]

[tool.pytest.ini_options]
addopts = "-q"
testpaths = ["tests"]
